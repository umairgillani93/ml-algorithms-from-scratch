WHAT WE DID SOFAR!!

- Self Attention:
  - matrix multiplication
  - softmax implementation
  - dk_square_root
  - transpose of Matrix

- Multi Head Attention:
  - Concatenating all self attention heads - Colum Majro
  - res->data[i * res->cols + (curr_col + j)] = m->data[i * m->cols + j];

WHAT WE NEED!!

- Token IDS
- Text Vocabulary
- Embedding Model
- Positional Embedding
- Layer Noram + Addition

Layers: 

- Embedding
- Multi-Head Attention
- Feed Forward Network (FFN)
- LayerNorm
- Transformer Block
- Full Transformer Model FFN

